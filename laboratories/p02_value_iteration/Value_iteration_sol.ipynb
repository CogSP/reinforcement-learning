{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73118f47",
      "metadata": {
        "id": "73118f47"
      },
      "source": [
        "# Value iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ec0869",
      "metadata": {
        "id": "53ec0869"
      },
      "source": [
        "![](imgs/value_iteration_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e70165",
      "metadata": {
        "id": "e7e70165"
      },
      "source": [
        "![](imgs/value_iteration_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5451f934",
      "metadata": {
        "id": "5451f934"
      },
      "source": [
        "![](imgs/value_iteration_3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e52736fc",
      "metadata": {
        "id": "e52736fc"
      },
      "source": [
        "![](imgs/value_iteration_4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09972ce1",
      "metadata": {
        "id": "09972ce1"
      },
      "source": [
        "![](imgs/value_iteration_5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dfc96a0",
      "metadata": {
        "id": "5dfc96a0"
      },
      "source": [
        "![](imgs/value_iteration_6.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "SUkadH__i-OK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUkadH__i-OK",
        "outputId": "c6cb3a27-b2f5-48c7-c6ed-88a08732f522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2523810d",
      "metadata": {
        "id": "2523810d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import time\n",
        "from gymnasium import spaces\n",
        "import os\n",
        "\n",
        "\n",
        "# custom 2d grid world enviroment\n",
        "class GridWorld(gym.Env):\n",
        "    metadata = {'render.modes': ['console']}\n",
        "\n",
        "    # actions available\n",
        "    UP = 0\n",
        "    LEFT = 1\n",
        "    DOWN = 2\n",
        "    RIGHT = 3\n",
        "\n",
        "\n",
        "    def __init__(self, width, height):\n",
        "        super(GridWorld, self).__init__()\n",
        "        self.ACTION_NAMES = [\"UP\", \"LEFT\", \"DOWN\", \"RIGHT\"]\n",
        "        self.num_actions = 4\n",
        "\n",
        "        self.size = width * height  # size of the grid world\n",
        "        self.num_states = self.size\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.num_obstacles = int((width+height)/2)\n",
        "        self.end_state = np.array([height - 1, width - 1], dtype=np.uint8) # goal state = bottom right cell\n",
        "\n",
        "        # actions of agents : up, down, left and right\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        # observation : cell indices in the grid\n",
        "        self.observation_space = spaces.MultiDiscrete([self.height, self.width])\n",
        "\n",
        "        self.obstacles = np.zeros((height, width))\n",
        "\n",
        "        for i in range(self.num_obstacles):\n",
        "            self.obstacles[ random.randrange(height) , random.randrange(width)] = 1\n",
        "\n",
        "        self.num_steps = 0\n",
        "        self.max_steps = height*width\n",
        "\n",
        "        self.current_state = np.zeros((2), np.uint8)#init state = [0,0]\n",
        "\n",
        "        self.directions = np.array([\n",
        "            [-1,0], #UP\n",
        "            [0,-1], #LEFT\n",
        "            [1,0], #DOWN\n",
        "            [0,1] #RIGHT\n",
        "        ])\n",
        "\n",
        "    def transition_function(self, s, a):\n",
        "        s_prime = s + self.directions[a,:]\n",
        "\n",
        "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
        "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
        "                return s_prime\n",
        "\n",
        "        return s\n",
        "\n",
        "    def transition_probabilities(self, s, a):\n",
        "        prob_next_state = np.zeros((self.heigth, self.width))\n",
        "        s_prime = self.transition_function(s, a)\n",
        "\n",
        "        # in each position you will have probability 0\n",
        "        # in position s', the next state, found with the transition function\n",
        "        # you will have 1\n",
        "        # it means that this is a deterministic grid world\n",
        "        prob_next_state[s_prime[0], s_prime[1]] = 1.0\n",
        "\n",
        "        return prob_next_state #.flatten()\n",
        "\n",
        "    def reward_function(self,s):\n",
        "        r = 0\n",
        "        if (s == self.end_state).all():\n",
        "            r = 1\n",
        "\n",
        "        return r\n",
        "\n",
        "    def termination_condition(self, s):\n",
        "\n",
        "        terminated = (s == self.end_state).all()\n",
        "        truncated = self.num_steps > self.max_steps\n",
        "\n",
        "        return terminated, truncated\n",
        "\n",
        "    def step(self, action):\n",
        "        s_prime = self.transition_function(self.current_state, action)\n",
        "        reward = self.reward_function(s_prime)\n",
        "        terminated, truncated = self.termination_condition(s_prime)\n",
        "\n",
        "        self.current_state = s_prime\n",
        "        self.num_steps += 1\n",
        "\n",
        "        return self.current_state, reward, terminated, truncated, None\n",
        "\n",
        "    def render(self):\n",
        "        '''\n",
        "            render the state\n",
        "        '''\n",
        "\n",
        "        row = self.current_state[0]\n",
        "        col = self.current_state[1]\n",
        "\n",
        "        for r in range(self.height):\n",
        "            for c in range(self.width):\n",
        "                if r == row and c == col:\n",
        "                    print(\"| A \", end='')\n",
        "                elif r == self.end_state[0] and c == self.end_state[1]:\n",
        "                    print(\"| G \", end='')\n",
        "                else:\n",
        "                    if self.obstacles[r,c] == 1:\n",
        "                        print('|///', end='')\n",
        "                    else:\n",
        "                        print('|___', end='')\n",
        "            print('|')\n",
        "        print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = np.zeros((2), np.uint8)\n",
        "        self.num_steps = 0\n",
        "        return self.current_state\n",
        "\n",
        "\n",
        "    # computes the map of the rewards: a table\n",
        "    # that associate the reward to each state of the grid world\n",
        "    def reward_probabilities(self):\n",
        "        rewards = np.zeros((self.num_states))\n",
        "        i = 0\n",
        "        for r in range(self.height):\n",
        "            for c in range(self.width):\n",
        "                state = np.array([r,c], dtype=np.uint8)\n",
        "                rewards[i] = self.reward_function(state)\n",
        "                i+=1\n",
        "\n",
        "        return rewards\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class NonDeterministicGridWorld(GridWorld):\n",
        "    def __init__(self, width, height, p=0.8):\n",
        "        super(NonDeterministicGridWorld, self).__init__(width, height)\n",
        "        self.probability_right_action = p\n",
        "\n",
        "    def transition_function(self, s, a):\n",
        "        s_prime = s + self.directions[a, :]\n",
        "\n",
        "        #with probability 1 - p diagonal movement\n",
        "        if random.random() <= 1 - self.probability_right_action:\n",
        "            if random.random() < 0.5:\n",
        "                s_prime = s_prime + self.directions[(a+1)%self.num_actions, :]\n",
        "            else:\n",
        "                s_prime = s_prime + self.directions[(a-1)%self.num_actions, :]\n",
        "\n",
        "\n",
        "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
        "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
        "                return s_prime\n",
        "\n",
        "        return s\n",
        "\n",
        "    def transition_probabilities(self, s, a):\n",
        "        cells = []\n",
        "        probs = []\n",
        "\n",
        "        # this associate to each state the probability of ending up there from s appling\n",
        "        # action a. This time, since the grid world is non-deterministic\n",
        "        # you won't want just 0 and 1\n",
        "        prob_next_state = np.zeros((self.height, self.width))\n",
        "\n",
        "        # right in the sense of \"correct\"\n",
        "        # this will insert the probability of ending up in the intended cell inside the prob_next_state matrix\n",
        "        s_prime_right =  s + self.directions[a, :]\n",
        "        if s_prime_right[0] < self.height and s_prime_right[1] < self.width and (s_prime_right >= 0).all():\n",
        "            if self.obstacles[s_prime_right[0], s_prime_right[1]] == 0:\n",
        "                prob_next_state[s_prime_right[0], s_prime_right[1]] = self.probability_right_action\n",
        "                cells.append(s_prime_right)\n",
        "                probs.append(self.probability_right_action)\n",
        "\n",
        "        # these two above are the probability of ending up in the random diagonally-chosen cell\n",
        "        s_prime = s_prime_right + self.directions[(a + 1) % self.num_actions, :]\n",
        "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
        "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
        "                prob_next_state[s_prime[0], s_prime[1]] = (1 - self.probability_right_action) / 2\n",
        "                cells.append(s_prime.copy())\n",
        "                probs.append((1 - self.probability_right_action) / 2)\n",
        "\n",
        "        s_prime = s_prime_right + self.directions[(a - 1) % self.num_actions, :]\n",
        "        if s_prime[0] < self.height and s_prime[1] < self.width and (s_prime >= 0).all():\n",
        "            if self.obstacles[s_prime[0], s_prime[1]] == 0 :\n",
        "                prob_next_state[s_prime[0], s_prime[1]] = (1 - self.probability_right_action) / 2\n",
        "                cells.append(s_prime.copy())\n",
        "                probs.append((1 - self.probability_right_action) / 2)\n",
        "\n",
        "        #normalization\n",
        "        sump = sum(probs)\n",
        "        #for cell in cells:\n",
        "        #    prob_next_state[cell[0], cell[1]] /= sump\n",
        "        prob_next_state[s[0], s[1]] = 1 - sump\n",
        "        return prob_next_state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "602977ce",
      "metadata": {
        "id": "602977ce"
      },
      "source": [
        "To apply value iteration we need the **transition probabilities** and the **reward function**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01dddba2",
      "metadata": {
        "id": "01dddba2"
      },
      "source": [
        "Print the probability over the next state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "378bbbdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "378bbbdd",
        "outputId": "af441ea5-c8bc-46e0-edd5-0cbb3335c939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| A |///|___|\n",
            "|___|___|___|\n",
            "|___|___|///|\n",
            "|___|___|___|\n",
            "|___|///| G |\n",
            "\n",
            "\n",
            "[[0.1 0.  0. ]\n",
            " [0.8 0.1 0. ]\n",
            " [0.  0.  0. ]\n",
            " [0.  0.  0. ]\n",
            " [0.  0.  0. ]]\n"
          ]
        }
      ],
      "source": [
        "env = NonDeterministicGridWorld(3,5)\n",
        "state = env.reset()\n",
        "env.render()\n",
        "# next state if we start from state 0,0 and we do action down\n",
        "next_state_prob = env.transition_probabilities(state, 2)\n",
        "print(next_state_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd61c23d",
      "metadata": {
        "id": "fd61c23d"
      },
      "source": [
        "reward values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d9826985",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9826985",
        "outputId": "8c01c103-805a-4dd5-ab6d-3dc3687a09e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ],
      "source": [
        "print(env.reward_probabilities())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c71dea30",
      "metadata": {
        "id": "c71dea30"
      },
      "source": [
        "# Value iteration algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff5512d",
      "metadata": {
        "id": "5ff5512d"
      },
      "source": [
        "![](imgs/value_iteration.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "408efa64",
      "metadata": {
        "id": "408efa64"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, gamma=0.99, iters=100):\n",
        "    # initialize values\n",
        "    values = np.zeros((env.num_states))\n",
        "    best_actions = np.zeros((env.num_states), dtype=int)\n",
        "\n",
        "    # this is a 15 x 2, because each state has two coordinates\n",
        "    STATES = np.zeros((env.num_states, 2), dtype=np.uint8)\n",
        "    REWARDS = env.reward_probabilities()\n",
        "    print(REWARDS)\n",
        "    i = 0\n",
        "    for r in range(env.height):\n",
        "        for c in range(env.width):\n",
        "            state = np.array([r, c], dtype=np.uint8)\n",
        "            STATES[i] = state\n",
        "            i += 1\n",
        "    delta = 1\n",
        "    for i in range(iters):\n",
        "        v_old = values.copy()\n",
        "\n",
        "        # for each state\n",
        "        for s in range(env.num_states):\n",
        "            state = STATES[s]\n",
        "\n",
        "            if (state == env.end_state).all() or i >= env.max_steps:\n",
        "                continue # if we reach the termination condition, we cannot perform any action\n",
        "\n",
        "\n",
        "            max_va = -np.inf\n",
        "            best_a = 0\n",
        "\n",
        "            # for the current state s, compute the best action as the one that maximizes v\n",
        "            # and update values[s] and best_action[s]\n",
        "            for a in range(env.num_actions):\n",
        "                next_state_prob = env.transition_probabilities(state, a).flatten()\n",
        "\n",
        "                # Bellman equation for v\n",
        "                va = (next_state_prob*(REWARDS + gamma*v_old)).sum()\n",
        "\n",
        "                if va > max_va:\n",
        "                    max_va = va\n",
        "                    best_a = a\n",
        "            values[s] = max_va\n",
        "            best_actions[s] = best_a\n",
        "\n",
        "\n",
        "    # finally, having updated the best action and its value v for each state s, we return the \"map\"\n",
        "    return values.reshape((env.height, env.width)), best_actions.reshape((env.height, env.width))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33dcbb0d",
      "metadata": {
        "id": "33dcbb0d"
      },
      "source": [
        "estimate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1dfa419c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dfa419c",
        "outputId": "f05d3325-65e0-4f93-8ecf-6957d8f432d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "[[0.94830519 0.95441123 0.94676876]\n",
            " [0.95809921 0.96575885 0.95635226]\n",
            " [0.96774609 0.97770444 0.98537072]\n",
            " [0.97574406 0.98781902 0.99750623]\n",
            " [0.96624123 0.99750623 0.        ]]\n",
            "[[2 2 2]\n",
            " [2 2 1]\n",
            " [3 2 2]\n",
            " [3 3 2]\n",
            " [0 3 0]]\n"
          ]
        }
      ],
      "source": [
        "values, best_actions = value_iteration(env)\n",
        "\n",
        "print(values)\n",
        "print(best_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a68ae40",
      "metadata": {
        "id": "0a68ae40"
      },
      "source": [
        "simulate optimal policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "edfe0c92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edfe0c92",
        "outputId": "4bca4f39-a6a7-45f1-cd83-6f830fca7cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|___|///|___|\n",
            "| A |___|___|\n",
            "|___|___|///|\n",
            "|___|___|___|\n",
            "|___|///| G |\n",
            "\n",
            "\n",
            "|___|///|___|\n",
            "|___|___|___|\n",
            "| A |___|///|\n",
            "|___|___|___|\n",
            "|___|///| G |\n",
            "\n",
            "\n",
            "|___|///|___|\n",
            "|___| A |___|\n",
            "|___|___|///|\n",
            "|___|___|___|\n",
            "|___|///| G |\n",
            "\n",
            "\n",
            "|___|///|___|\n",
            "|___|___|___|\n",
            "|___| A |///|\n",
            "|___|___|___|\n",
            "|___|///| G |\n",
            "\n",
            "\n",
            "|___|///|___|\n",
            "|___|___|___|\n",
            "|___|___|///|\n",
            "|___| A |___|\n",
            "|___|///| G |\n",
            "\n",
            "\n",
            "|___|///|___|\n",
            "|___|___|___|\n",
            "|___|___|///|\n",
            "|___|___| A |\n",
            "|___|///| G |\n",
            "\n",
            "\n",
            "|___|///|___|\n",
            "|___|___|___|\n",
            "|___|___|///|\n",
            "|___|___|___|\n",
            "|___|///| A |\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "done = False\n",
        "state = env.reset()\n",
        "\n",
        "# once you have the best action to perform for each\n",
        "# state you are done: just always choose the best\n",
        "# action to perform in the state you are currently\n",
        "# in and finally you will reach the goal\n",
        "while not done:\n",
        "    action = best_actions[state[0],state[1]]\n",
        "\n",
        "    state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    env.render()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}